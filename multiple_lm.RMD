---
title: "Multiple Linear Regression & Regularization"
author: "Jeffrey Strickland"
date: "2022-12-11"
output:
  word_document: 
    reference_docx: C:\Users\jeff\Documents\R\book_template.docx
  always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
#The Multiple Linear Regression Model 

##Notation for the Population Model

A population model for a multiple linear regression model that relates a $y$-variable to $p-1$ $x$-variables is written as

$$y_i=β_0+β_1 x_{i,1}+β_2 x_{I ,2}+⋯+β_{p-1} x_{I,p-1}+ ε_i$$

We assume that the $ε_i$ have a normal distribution with mean 0 and constant variance $σ^2$. These are the same assumptions that we used in simple regression with one $x$-variable. 

The subscript $i$ refers to the $i^{th}$ individual or unit in the population. In the notation for the $x$-variables, the subscript following $i$ simply denotes which $x$-variable it is. 

The word "linear" in "multiple linear regression" refers to the fact that the model is *linear in the parameters*, $β_0,β_1,…,β_{p-1}$. This simply means that each parameter multiplies an $x$-variable, while the regression function is a sum of these "parameter times $x$-variable" terms. Each x-variable can be a predictor variable or a transformation of predictor variables (such as the square of a predictor variable or two predictor variables multiplied together). Allowing non-linear transformation of predictor variables like this enables the multiple linear regression model to represent non-linear relationships between the response variable and the predictor variables. Note that even $β_0$ represents a "parameter times x-variable" term if you think of the $x$-variable that is multiplied by $β_0$ as being the constant function "1." 

The model includes $p-1$ $x$-variables, but $p$ regression parameters (beta) because of the intercept term $β_0$.

## Estimates of the Model Parameters

The estimates of the $β$ parameters are the values that minimize the sum of squared errors for the sample. The exact formula for this is given in the next section on matrix notation.

The letter $b$ is used to represent a sample estimate of a parameter $β$. Thus, $b_0$ is the sample estimate of $β_0$, $b_1$ is the sample estimate of $β_1$, and so on.

$MSE=SSE/(n-p)$ (mean square error) estimates $σ^2$, the variance of the errors. In the formula, $n$ = sample size, $p$ = number of parameters in the model (including the intercept) and $SSE=$ sum of squared errors. Notice that for simple linear regression $p=2$. Thus, we get the formula for MSE that we introduced in that context of one predictor.

$S=\sqrt{MSE}$ or RMSE (root mean square error) estimates $σ$ and is known as the regression standard error or the residual standard error.
In the case of two predictors, the estimated regression equation yields a plane (as opposed to a line in the simple linear regression setting). For more than two predictors, the estimated regression equation yields a hyperplane.

## Interpretation of the Model Parameters

Each $β$ parameter represents the change in the mean response, $E(y)$, per unit increase in the associated predictor variable when all the other predictors are held constant. 
For example, $β_1$ represents the estimated change in the mean response, $E(y)$, per unit increase in $x_1$ when $x_2,x_3,…,x_{p-1}$ are held constant. 
The intercept term, $β_0$, represents the estimated mean response, $E(y)$, when all the predictors $x_1,x_2,…,x_{p-1}$, are all zero (which may or may not have any practical meaning).

## Predicted Values and Residuals

A predicted value is calculated as $\hat{y}_i=b_0+b_1 x_{i,1}+b_2 x_{i,2}+⋯+b_{p-1}x_{i,p-1}$, where the b values come from statistical software and the $x$-values are specified by us. 

A **residual (error)** term is calculated as $e_i=y_i-\hat{y}_i$, the difference between an actual and a predicted value of $y$.

A plot of residuals (vertical) versus predicted values (horizontal) ideally should resemble a horizontal random band. Departures from this form indicates difficulties with the model and/or data.

Other residual analyses can be done exactly as we did in simple regression. For instance, we might wish to examine a normal probability plot (NPP) of the residuals. Additional plots to consider are plots of residuals versus each $x$-variable separately. This might help us identify sources of curvature or nonconstant variance.

**ANOVA Table** 
|  Source          |  df |  SS|             MS|   F|
|:-----------------|----:|---:|--------------:|---:|
|Regression        |$p-1$| SSR|$MSR=SSR/(p-1)$| 110|
|Error             |$n-p$| SSE|$MSE=SSE/(n-p)$| 110|
|Total             |$n-1$|SSTO|               |    |

## Coefficient of Determination, R-squared, and Adjusted R-squared

As in simple linear regression, $R^2=\frac{SSR}{SSTO}=1-\frac{SS}{SSTO}$, and represents the proportion of variation in y (about its mean) "explained" by the multiple linear regression model with predictors, $x_1,x_2,…$.
If we start with a simple linear regression model with one predictor variable, $x_1$, then add a second predictor variable, $x_2$, SSE will decrease (or stay the same) while SSTO remains constant, and so $R^2$ will increase (or stay the same). In other words, $R^2$ always increases (or stays the same) as more predictors are added to a multiple linear regression model, even if the predictors added are unrelated to the response variable. Thus, by itself, $R^2$ cannot be used to help us identify which predictors should be included in a model and which should be excluded.

An alternative measure adjusted $R^2$, does not necessarily increase as more predictors are added, and can be used to help us identify which predictors should be included in a model and which should be excluded. The adjusted $R$-squared is $R^2=1-(\frac{n-1}{n-p})(1-R^2)$, and, while it has no practical interpretation, is useful for such model building purposes. Simply stated, when comparing two models used to predict the same response variable, we generally prefer the model with the higher value of adjusted $R^2$.

## Significance Testing of Each Variable

Within a multiple regression model, we may want to know whether a particular $x$-variable is making a useful contribution to the model. That is, given the presence of the other $x$-variables in the model, does a particular $x$-variable help us predict or explain the $y$-variable? For instance, suppose that we have three $x$-variables in the model. The general structure of the model could be
$$y=β_0+β_1 x_1+β_2 x_2+⋯+β_3 x_3+ ε$$
As an example, to determine whether variable $x_1$ is a useful predictor variable in this model, we could test

$$H_0:β_1=0$$
$$H_A:β_1≠0$$

If the null hypothesis above were the case, then a change in the value of $x_1$ would not change $y$, so $y$ and $x_1$ are not linearly related (considering $x_2$ and $x_3$). Also, we would still be left with variables and being present in the model. When we cannot reject the null hypothesis above, we should say that we do not need variable $x_1$ in the model given that variables $x_2$ and $x_3$ will remain in the model. In general, the interpretation of a slope in multiple regression can be tricky. Correlations among the predictors can change the slope values dramatically from what they would be in separate simple regressions.

To carry out the test, statistical software will report p-values for all coefficients in the model. Each p-value will be based on a t-statistic calculated as

$$t^*=(sample coefficient-hpothesized vale)/(standard error of coefficient)$$

For our example above, the t-statistic is:

$$t^*=\frac{b_1-0}{se(b_1} =\frac{b_1}{se(b_1)}$$.

Note that the hypothesized value is usually just 0, so this portion of the formula is often omitted.

Multiple linear regression, in contrast to simple linear regression, involves multiple predictors and so testing each variable can quickly become complicated. For example, suppose we apply two separate tests for two predictors, say $x_1$ and $x_2$, and both tests have high $p$-values. One test suggests $x_1$ is not needed in a model with all the other predictors included, while the other test suggests $x_2$ is not needed in a model with all the other predictors included. But this doesn't necessarily mean that both $x_1$ and $x_2$ are not needed in a model with all the other predictors included. It may well turn out that we would do better to omit either $x_1$ or $x_2$ from the model, but not both.

## Multiple Linear Regression: Satellite Survivability

Aside from potential nation-state threats, what factors might affect the survivability of resident space objects (i.e., satellites)? We constructed a database comprised of 10 factors and a response variable using simulation. The data is purely hypothetical. The satellites used may not represent reality, although the names are real. The dataset is named *“Survive”* for short, and is comma delimited.

## Data Preprocessing

First, we need to perform some preprocessing tasks on the data to prepare it for modeling.

### Load Required R Packages.

First we load the $R$ packages that we anticipate needing. We may find that we need additional packages later.

```{r}
#install.packages("rsq").
library(rsq)
library(ggplot2)
library(hrbrthemes)
```

### Import the Data

Next, we’ll load the comma delimited data using the `read.csv` function and print the first 6 rows, including the headings.

```{r}
train <-read.csv("C:\\Users\\jeff\\Documents\\Data\\Survive.csv")
head(train,6) 
```

In the dataset, we see characteristics of the RSOs (Weight, Visibility, etc.), characteristics of their orbits (Height, Type, etc.), and a survivability score. We want to predict survivability based on these “features”.

### Fill Missing Values

Now, let’s impute missing values (i.e., fill missing spaces) with “NA”.

```{r}
train[train==""] <- NA #Filling blank values with NA.
names(train)
```

### Create Dataset with Numeric Features

Next, let’s create two datasets, one with the numeric independent variables, $X$, and one with the dependent variable or response, $Y$. $X$ will be comprised of columns 4, 6 and 8, or `RSO_Visibility`, `RSO_MRP` and `Orbit_Estab_Year`, respectively. $Y$ is the response, `Survivability`. We'll also print the names of the variables in each set to confirm we have the right ones.

```{r}
X <- train[c(4,6,8)] 
names((X))

Y <- train[c(12)] #Storing the dependent variable.
names((Y))
```

### Train-Test Split

Here, we split the datasets, $X$ and $Y$, into two sets each. The training datasets will be comprised of 70% of the data from each set, and the cross-validation sets will be comprised of the remaining 30%.

```{r}
set.seed(567) 
part <- sample(2, nrow(X), replace = TRUE, prob = c(0.7, 0.3))
X_train <- X[part == 1,]
X_cv <- X[part == 2,]

Y_train <- Y[part == 1,]
Y_cv <- Y[part == 2,]
```

## Model 1: Lnear Model with Numeric Features

We'll train model 1 using two of the numeric features, `RSO_MRP` and `Orbit_Estab_Year`. We'll also exclude the intercept term by adding `+0` to the regression formula.

```{r}
model1 <- lm(Y_train ~ RSO_MRP + Orbit_Estab_Year + 0, data = X_train ) 
summary(model1)
```

### Extract Model 1 Equation

Next, we use the `R` function `extract_eq` from the `equatiomatic` package, to extract the `Model 1` equation.

```{r}
library(equatiomatic)
equatiomatic::extract_eq(model1)
```

### Predict using Model 1

Now, we use the model and the cross-validation set to make predictions.

```{r}
predict_1 <- predict(model1, X_cv) #Predicting the values.
```

### Calculate Model 1 Metrics

Next, we calculate the Model 1 performance metrics, RMSE and R-squared.

Now, let’s calculate MSE, based on the model prediction, and R-squared based on the model. MSE or Mean Square Error, $R^2$ is a metric that determines how much of the total variation in $Y$ (dependent variable) is explained by the variation in $X$ (independent variable). Mathematically, it can be written as:

$$R^2=1-(\frac{\sum(Y_Actual-Y_Predicted )^2)}{(\sum(Y_Actual-Y_Mean )^2 )}$$

The value of $R^2$ is always between 0 and 1, where 0 means that the model does not explain any variability in the response variable and 1 meaning it explains full variability in the response variable.

**Root Mean Square Error:** In `R`, the root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis.

In other words, how concentrated the data around the line of best fit.

$$RMSE=\sqrt{\frac{\sum(P_i–O_i )^2}{n}}$$

where:
$\sigma$ symbol indicates “sum”

$P_i$ is the predicted value for the $i^{th}$ observation in the dataset

$O_i$ is the observed value for the $i^{th}$ observation in the dataset

$n$ is the sample size

Now let’s calculate these model metrics.

```{r}
library(Metrics)
RMSE <- rmse(Y_cv, predict_1)
R.sq <- rsq(model1)
print(paste("R-square =", R.sq))
print(paste("RMSE =", RMSE))
```

We’ll use MSE for `Model 1` for comparison with subsequent models. In this case, $R^2$ is 0.75, meaning, 75% of variance in survivability is explained by year of establishment and MRP. In other words, if you know year of establishment and the MRP, you’ll have 75% of the information to make an accurate prediction about survivability. 

### Plot Model 1 Coefficients

Finally, we plot the `Model 1` coefficient values using a `barplot`. Let's look at the coefficients of this linear regression model. Since `RSO_MRP` has a high coefficient, having higher *RSO MRP* scores contributes to higher survivability.

```{r, fig.width = 8, fig.height = 5, dpi = 300}
par(mar=c(11,4,2,1))
barplot(model1$coefficients, main = "Model 1 Coefficients", 
        ylab = "Coefficients", las = 2, col = 'skyblue')
```

## Model 2: Lnear Model with Numeric Features

We'll train `Model 1` using two of the numeric features, `RSO_MRP` and `Orbit_Estab_Year` as before with the intercept excluded, but we add `RSO_Visibility`. 

```{r}
model2 <- lm(Y_train ~ RSO_MRP + Orbit_Estab_Year + RSO_Visibility + 0, data = X_train ) 
summary(model1)
```

The model summary shows that all three features are significant as predictors, so let’s get some predictions and check the MSE.

### Extract Model 2 Equation

Next, we use the `R` function `extract_eq` from the `equatiomatic` package, to extract the `Model 2` equation.

```{r}
library(equatiomatic)
equatiomatic::extract_eq(model2)
```

### Predict using Model 2

Now, we use the model and the cross-validation set to make predictions.

```{r}
predict_2 <- predict(model2, X_cv) #Predicting the values.
```

### Calculate Model 2 Metrics

Next, we calculate the `Model 2` performance metrics, `RMSE` and `R-squared`.

```{r}
library(Metrics)
RMSE <- rmse(Y_cv, predict_2)
R.sq <- rsq(model2)
print(paste("R-square =", R.sq))
print(paste("RMSE =", RMSE))
```

## Model 2 Performance

The RMSE of 1320.721 did not change very much but is slightly larger than 1320.481 from Model 1. `Model 2` introduces slightly more error. $R^2$ is 0.75 and about the same as `Model 1`’s. Moreover, we have learned from the residuals vs. fitted value plots that the error variance is increasing implying that the regression model is not a good one. So, back to the drawing board.

### Plot Model 2 Coefficients

Finally, we plot the `Model 2` coefficient values using a `barplot`.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
par(mar=c(11,4,2,1))
barplot(model2$coefficients, main = "Model 2 Coefficients", 
        ylab = "Coefficients", las = 2, col = 'skyblue')
```

Figure 3-1. Bar plot for Model 2 coefficients

### Model 2 Residual Analysis

Recall that residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. Thus, residuals represent the portion of the validation data not explained by the model.

## Residual Plots

Residual analysis plots show different information depending on whether you use time-domain or frequency-domain input-output validation data. For frequency-domain validation data, the plot shows the following two axes:

1.	Estimated power spectrum of the residuals for each output
2.	Transfer-function amplitude from the input to the residuals for each input-output pair

For time-domain validation data, the plot shows the following two axes:

1.	Autocorrelation function of the residuals for each output
2.	Cross-correlation between the input and the residuals for each input-output pair

For linear models, you can estimate a model using time-domain data, and then validate the model using frequency domain data.

## Displaying the Confidence Interval

The confidence interval corresponds to the range of residual values with a specific probability of being statistically insignificant for the system.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
plot(model2, col = "skyblue")
```

The first residual plot looks like a funnel shape. This shape indicates *heteroskedasticity*. The presence of non-constant variance in the error terms results in heteroskedasticity. We can clearly see that the variance of error terms(residuals) is not constant. Generally, non-constant variance arises in presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.

We can easily check this by looking at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern as shown above. This indicates signs of non-linearity in the data which has not been captured by the model. Also in the first plot, we see three outliers: data points, 79, 96, and 155. Typically, we would remove these points from the dataset, but given the amount of data we have, they do not have a great impact on the model’s fit.

The second plot is the Normal Q-Q plot. The concern here is curvature of the data. If the model were a good fit, the data would be more linear.
The third plot shows the standardized residuals and does not provide any additional value to those shown in the first plot.

The fourth plot shows *Cook’s Distance*. Cook’s Distance is an estimate of the influence of a data point. It considers both the leverage and residual of each observation. Cook’s Distance is a summary of how much a regression model changes when the $i^{th}$ observation is removed.

### Import the Data

```{r}
train <-read.csv("C:\\Users\\jeff\\Documents\\Data\\Survive.csv")
summary(train)
```

### Select Features

For `Model 3`, we select most of the features, categorical and numeric, for the dataset $X$.

```{r}
X <- train[c(2,3,4,5,6,8,9,10,11)]
names(X)
```

### Fill Missing Feature Values

Next, we fill missing feature values using means for means for numeric fields and the lowest categories (most common) for categorical features.

```{r}
X$RSO_Weight[is.na(X$RSO_Weight)] <- mean(X$RSO_Weight, na.rm = TRUE)
X$RSO_Density[is.na(X$RSO_Density)] <- "Low"
X$RSO_Visibility[X$RSO_Visibility == 0] <- mean(X$RSO_Visibility)
X$RSO_MRP[is.na(X$RSO_MRP)] <- mean(X$RSO_MRP, na.rm = TRUE)
X$Orbit_Estab_Year=2013 - X$Orbit_Estab_Year
X$Orbit_Height[is.na(X$Orbit_Height)] <- "LEO"
X$Stealth_Type[is.na(X$Stealth_Type)] <- "Stealth_1"
X$RSO_Type[is.na(X$RSO_Type)] <- "RSO_Type1"
```

### Create Response Set

We also create the response set $Y$ (survivability) and fill missing values with the mean value.

```{r}
Y <- train[c(12)]
Y$Survivability[is.na(Y$Survivability)] <- mean(Y$Survivability, na.rm = TRUE) 
names((Y))
```

### Train-Test Split

Next, we split $X$ and $Y$ into training and cross-validation sets. 

```{r}
set.seed(567)
part <- sample(2, nrow(X), replace = TRUE, prob = c(0.7, 0.3))
X_train <- X[part == 1,]
X_cv <- X[part == 2,]

Y_train <- Y[part == 1,]
Y_cv <- Y[part == 2,]
```

## Model 3: Mixed Feature Types

For `Model 3`, we add the categorical values, and the model we build has mixed feature types.

```{r}
model3x <- lm(Y_train ~ RSO_MRP + Orbit_Height + RSO_Visibility + Orbit_Estab_Year + Stealth_Type + 0, 
             data = X_train )
summary(model3x)
```

### Extract Model 3 Equation

Next, we use the `R` function extract_eq from the `equatiomatic` package, to extract the `Model 3` equation. The `equatiomatic` package provides the function for expressing the regression model in it mathematical format.

```{r}
equatiomatic::extract_eq(model3x)
```

### Use Model 3 to Predict

Now, we use the model and the cross-validation set to make predictions.

```{r}
predict_3 <- predict(model3x, X_cv)
```

### Model 3 Metrics

Next, we calculate the `Model 3` performance metrics, `RMSE` and `R-squared`.

```{r}
RMSE <- rmse(Y_cv, predict_3)
R.sq <- rsq(model3x)
print(paste("R-square =", R.sq))
print(paste("RMSE =", RMSE))
```

# Plot Model 3 Coefficients

Finally, we plot the `Model 3` coefficient values using a `barplot`.

```{r, fig.width = 6, fig.height = 5, dpi = 300}
par(mar=c(11,4,2,1))
barplot(model3x$coefficients, main = "Model 2 Coefficients", 
        ylab = "Coefficients", las = 2, col = 'blueviolet')
```
Figure 3-24. Bar plot of the model’s coefficient values.

### Model 3 Residual Analysis

Recall that residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. Thus, residuals represent the portion of the validation data not explained by the model.

### Model 3 Residual Plots

Residual analysis plots show different information depending on whether you use time-domain or frequency-domain input-output validation data. For linear models, you can estimate a model using time-domain data, and then validate the model using frequency domain data.

## Displaying the Confidence Interval

The confidence interval corresponds to the range of residual values with a specific probability of being statistically insignificant for the system.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
plot(model3x, col = "green2")
```
Figure 3-9. Plot of the residuals vs fitted values, with outliers at 79, 96, 155.

Figure 3-10. Normality plot showing a lack of fit in the upper quantiles, with outliers at 79, 96, 155.

Figure 3-11. Scale-location chart with standardized residuals, with outliers at 79, 96, 155.

Figure 3-12. Plot of standardized residuals vs model leverage, with outliers at 69, 295, 2242.

The first residual plot looks like a funnel shape. This shape indicates heteroskedasticity. The presence of non-constant variance in the error terms results in heteroskedasticity. We can clearly see that the variance of error terms (residuals) is not constant. Generally, non-constant variance arises in presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.

We can easily check this by looking at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern as shown above. This indicates signs of non-linearity in the data which has not been captured by the model.

Also in the first plot, we see three outliers: data points, 79, 96, and 155. Typically, we would remove these points from the dataset, but given the amount of data we have, they do not have a great impact on the model’s fit.
The second plot is the Normal Q-Q plot. The concern here is curvature of the data. If the model were a good fit, the data would be more linear.

The third plot shows the standardized residuals and does not provide any additional value to those shown in the first plot.

The fourth plot shows Cook’s Distance. Cook’s Distance is an estimate of the influence of a data point. It considers both the leverage and residual of each observation. Cook’s Distance is a summary of how much a regression model changes when the i^th observation is removed.

### How to Interpret a Residual Plot

When the non-linearity does not exist, we generally follow this process for interpreting residuals:

Step 1: Locate the residual = 0 line in the residual plot.

Step 2: Look at the points in the plot and answer the following questions:

Are they scattered randomly around the residual = 0 line? Or are they clustered in a curved pattern, such as a U-shaped pattern?

If the points show no pattern, that is, the points are randomly dispersed, we can conclude that a linear model is an appropriate model. In other words, if the residuals are randomly scattered around the residual = 0, it means that a linear model approximates the data points well without favoring certain inputs.

If the points show a curved pattern, such as a U-shaped pattern, we can conclude that a linear model is not appropriate and that a non-linear model might fit better.

### Error variance Analysis

Residuals plot that has an increasing trend suggests that the error variance increases with the independent variable (see Figure 1); while a distribution that reveals a decreasing trend indicates that the error variance decreases with the independent variable. Neither of these distributions are constant variance patterns. The foregoing is also true when we have drifting in the error variance, as demonstrated in Figure 2. Therefore, they indicate that the assumption of constant variance is not likely to be true and the regression is not a good one. On the other hand, a horizontal-band pattern suggests that the variance of the residuals is constant.

### Increasing Error Variance

The generate the figure below, we simulated some basic data that would be demonstrate an increasing error variance.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
set.seed(77)
n <- 300
res_order <- seq(1, 3, length.out = n)
g <- sample(c("m", "f"), size = n, replace = TRUE)
msd <- ifelse(g=="m", 2*2.5, 2) * exp(1.5*res_order)/10
residuals <- (1.2 + 2.1*res_order - 1.5*(g == "m") + 
2.8*res_order*(g == "m") + rnorm(n, sd = msd))/10
d <- data.frame(residuals, res_order, g)
ggplot(d, aes(res_order, residuals)) + 
geom_point(size = 2, col = 4) + 
ggtitle("Residual that show Increasing Variance")
```
Figure 3-12. Residuals with increasing error variance

To generate the figure below, we simulated some basic data that would demonstrate drifting error variance.

### Conclusions from Residual Analysis

The plots reveal several issues. One concern is introduction of outliers at data points 79, 96, and 155, which has the effect of squeezing the data toward the horizontal axis. This makes it difficult to analyze the residuals, precisely. To correct this, we would normally remove the data outlier and repeat the modeling and analysis. However, in the next section, we are going to try a different modeling type. Another concern is the apparent, increasing error variance.
The fourth plots show *Cook’s Distance*. Cook’s Distance is an estimate of the influence of a data point. It considers both the leverage and residual of each observation. Cook’s Distance is a summary of how much a regression model changes when the $i^{th}$ observation is removed.

### Scatterplots

Before we delve into another model, let’s look at some more plots that may be useful for our analysis. The first plot we generate is for *RSO Weight* vs. *Survivability*, which is categorized by *RSO Name*.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
ggplot(X_train, aes(x = RSO_Visibility, y = Y_train, color = RSO_Name)) + 
    geom_point(size = 2) +
    theme_ipsum() +
  labs(subtitle = "RSO Weight Vs Survivability", 
       y = "Survivability", 
       x = "RSO Weight", 
       title = "Scatterplot")
```
Figure 3-13. Resident space object weight vs survivability.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
library(ggplot2)
gg <- ggplot(X_train, aes(x = RSO_Weight, y = Y_train, 
col = Stealth_Type)) + 
  geom_point(size = 2) + 
  theme_ipsum() +
  labs(subtitle = "RSO Weight Vs Survivability", 
       y = "Survivability", 
       x = "RSO Weight", 
       title = "Scatterplot")
gg
```
Figure 3-14.  Resident space object stealth type vs survivability.

We could expect, it appears the more advanced the stealth technology is, the more survivable is the RSO. However, note that are a few exceptions.
Now, we generate a scatterplot using *RSO Weight* vs. *Survivability*, but this time we categorize it by *RSO Density*.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = RSO_Weight, y = Y_train)) + 
  geom_point(aes(col = RSO_Density)) 
gg
```
Figure 3-15. Resident space object weight vs survivability by density.

The plot adds validity to the model outcomes, since *RSO Density* was not a significant feature in the model. So, now let’s look at *RSO MRP* vs. *Survivability*. Here, we categorize the data by *Stealth Type*.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = RSO_MRP, y = Y_train)) + 
  geom_point(aes(col = Stealth_Type)) 
gg
```
Figure 3-16. Resident space object MRO vs survivability by stealth type.

Although it is difficult to see all the data points for each *Stealth Type*, there does seem to be a pattern of increasing survivability with increasing suitability.
Now, let’s do the same analysis but categorized as *RSO Density* (recall that it is not significant in the model).

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = RSO_Density, y = Y_train)) + 
  geom_point(aes(col = RSO_Density)) 
gg
```
Figure 3-17. Resident space object MRP vs survivability by density.

Again, there does seem to be a pattern of increasing survivability with increasing suitability, even though *RSO Density* was not a significant feature.
Now, let’s look at *Establishment Year* vs. *Survivability*, categorized by *Orbit Height*.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = Orbit_Height, 
y = Y_train)) + 
  geom_point(aes(col = Orbit_Height)) 
gg
```
Figure 3-18. Resident space object orbit establishment year vs survivability by orbit height.

The salmon-colored data points are missing values, so one thing we learned is that we should impute the missing values or remove them from the data. The plot seems to demonstrate that the longer the RSO has been established, it is less survivable. Note that the father to the left an RSO is in the plot, it’s been in orbit for less time than those on the right. Also, not that the data is skewed, i.e., not symmetric.

Let’s do the same analysis but with Stealth Type as the category.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = Orbit_Estab_Year, 
y = Y_train)) + 
  geom_point(aes(col = Stealth_Type)) 
gg
```
Figure 3-19. Resident space object orbit establishment year vs survivability by stealth type

The plot tells the same story for *Orbit Establishment* vs. *Survivability*. Note that *Stealth Type 4* is a younger technology.

Now let’s switch gears a bit and look at a different type of scatterplot, which is available in the car-package. This kind of scatterplot shows the information provided in previous plots but adds a few more diagnostics.

First, we see a dashed line in the plot that represents the trend of the data and around it, in the shaded light blue areas, a confidence interval is added.
Second, there is a solid blue liner fit line slightly above the trend curve.

Finally, there are two box plots, one representing the survivability data and another representing the *Established Year* data. The bow plots clearly support the observation of skewness we made previously. We also see that a linear fit might not be the best one, although our observation is not conclusive.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
library(car)
scatterplot(X$Orbit_Estab_Year, Y$Survivability)
```
Figure 3-20. Resident space object orbit establishment year vs survivability.

The next scatterplot is for *RSO MRP* vs. *Survivability*. The plot supplies the same information as we saw previously but adds more. In this instance we can see that underlying trend is linear, but increasing, as well as the confidence interval. The boxplots show skewness in the data and point out the central tendency. This plot also implies that the linear regression model may not be the best modeling option.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
scatterplot(X$RSO_MRP, Y$Survivability)
```
**Figure 3-21**. Resident space object MRP vs survivability

Finally, we look at *RSO Weight* vs. *Survivability* again. The plot does not add much information, except that the fit may be curvilinear, implying again that linear regression may not lead to the best fit.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
scatterplot(X$RSO_Weight, Y$Survivability)
```
**Figure 3-22**. Resident space object weight vs survivability.

### Holistic Scatterplot

This final scatterplot is different from previous ones, in that we can see the data categorized by *Stealth Type* and observe the darker color blue tend line is curvilinear.

```{r, fig.width = 6, fig.height = 4, dpi = 300}
gg <- ggplot(X_train, aes(x = RSO_Weight, y = Y_train)) + 
  geom_point(aes(col = Stealth_Type)) + 
  geom_smooth(method = "loess", se = F) + 
  labs(subtitle = "RSO Weight Vs Survivability", 
       y = "Survivability", 
       x = "RSO Weight", 
       title = "Scatterplot", 
       caption = "Source: midwest")
gg
```
**Figure 3-23**.  Resident space object weight vs survivability by stealth type.

### Model 3 Evaluation

Aside from our earlier performance metrics and the diagnostics we performed with our plots, we’ll now look at some numerical metrics. Recall the MSE from `Model 2` is 1979562 and its $R^2$ value is 0.3322661. `Model 3`’s MSE is close to zero, which is a drastic improvement. Also, the $R^2$ of 1.0 shows that the model is essentially a perfect fit. But wait! Didn’t the plots reveal some potential issues? They did, and this is probably a model that over-fits the data!

```{r, fig.width = 6, fig.height = 5, dpi = 300}
par(mar=c(11,4,2,1))
barplot(model3x$coefficients, main = "Model 3 Coefficients", 
ylab = "Coefficients", las = 2, col = "aquamarine2")
```
**Figure 3-24**. Bar plot of the model’s coefficient values.

### Bar Plot Observations

We can see that magnitude of the coefficients for `Orbit_Identifier OUT013`, `Orbit_IdentifierOUT018`, `Orbit_IdentifierOUT027`, 1Orbit_Identifier OUT0451, 1Orbit_IdentifierOUT0491, `Orbit_HeightGEO`, `Orbit_HeightMEO`, `Stealth_Type2`, `Stealth_Type3`, and `RSO_Type3`, are much higher as compared to rest of the coefficients. Therefore, the survivability of an RSO would be more driven by these features or levels of features. This might indicate an over-fitted model and validates our prior observation.

How can we reduce the magnitude of coefficients in our model to correct for over-fitting? For this purpose, we have different types of regression techniques, which use regularization to overcome this problem. So let us discuss them in the next chapter.

## A Glimpse of Lasso Regression

### Regularization

From Chapter 5, recall  that when building a model, we want training and test errors to be as small as possible. To accomplish this, we used polynomial terms to add complexity to our models. 

Referring to Figure 6-1, the curve on top represents the total error of the of the trained model with the optimal complexity obtained where the dotted line crosses the black curve. Recall that increasing model complexity prevents underfitting, while more complex models may lead to overfitting, such that our model will no generalize well when we use the test data. We discussed how to find optimal complexity of a model to reduce both the fit error and prediction error. 

Another way we can address model training vs generalization is to examine what we call bias-variance tradeoff.  This is shown by the red and green curves in Figure 7-1. The method we use is regularization, which will take our existing model and fid its optimal complexity.

![Illustration of the bias-variance tradeoff along with the total complexity error.](C:\Users\jeff\Documents\R\Fig_9_01.TIF)

So, what do we mean by this term regularization? Recall the method by which our model learns the parameters from the data is to increase complexity while minimizing some cost function. As we saw for linear regression, the model would minimize the mean squared error, that is the error between the outcome variable and our predicted variable squared. We now introduce a new cost function for regularization:

$$M(w)+λR(w)$$

where,
$M(w)$ is the model error or original cost function
$R(w)$ is the function of estimated parameters
$λ$ is the regularization power parameter

The regularization portion of the new cost function is $R(w)$ multiplied by $λ$. We add this term to the original cost function so that we can increasingly penalize the model when it becomes too complex. Essentially, this will allow us to "dumb-down" the model. So, by strengthening our parameter weights, and strengthening our model parameters, we increase the value of cost function. However, we're trying to minimize the cost function, so we're not going to be able to fit the model as close to the training data, as we might like. So, we add a penalty.

### The Regularization Parameter

The $λ$ is going to add a penalty proportional to the size of the strength of these parameters, or some function of these parameters, namely $R(w)$. We'll examine more closely how $λR(w)$ can be a function of the parameters later on. The takeaway is that the larger we make $λ$, the more we penalize stronger parameters. And the more we penalize our model for being stronger and having stronger parameters, the less complex that model will be. This will cause us to try minimizing the strength of all of our parameters while minimizing our original cost function, $M(w)$.
So, increasing the regularization parameter $λ$ allows us to manage the complexity tradeoff, but increases the model bias as shown by the red curve in Figure 6-1. On the other hand, less regularization makes it so that the model is more complex and will increase the variance. That is, if increase the complexity of a model, we may overfit the model (on the training set), and we will also increase the variance (the green curve in Figure 6-1).

Now let's take this concept of $λ$ as it relates to the coefficients in of the polynomial graphs that we saw in Figure 5-25. The idea here is to find the true function that blue lines given our sample data. So, let's say that we are starting with a model that's just polynomial degree 15. Rather than testing different polynomials of degree 1, 5 and 15 we can introduce the blue $λ$ term here, and as illustrated in Figure 6-2.

J(β_0,β_1 )=1/2m ∑_(i=1)^m▒〖((β_0+β_1 x_obs^i )-y_obs^i )^2+λ∑_(j=1)^k▒β_j^2 〗

![The effect of using the λ-term rather than the polynomial degrees..](C:\Users\jeff\Documents\R\Fig_9_02.TIF)
 
**Figure 6-2**. The effect of using the $λ$-term rather than the polynomial degrees.

Now, let's think about regularization in the context of feature selection, or determining which features are important to include in our model. So, regularization is essentially going to be a form of feature selection. That is, regularization is going to take the contribution of each feature and eliminate or reduce features as it adds more weight to the penalty.

Now what happens with regards to the relationship between the $λ$-values and those standardized coefficients. Generally, as seen in Figure 6-3, moving to the right as $\lambda$ increases, the standardized coefficients should decrease, so there should be that inverse relationship. 

![Coefficient value change vs regularization (λ) change.](C:\Users\jeff\Documents\R\Fig_9_03.TIF)
 
**Figure 6-3**. Coefficient value change vs regularization ($λ$) change

We do see these ratings increase, as the other features are decreasing and that would just be something due to multicollinearity. At a certain point, we'll see they all start to decrease and they're all decreasing monotonically towards 0 once they reach a certain threshold. So, ideally as we increase alpha, the value we choose when  working with *sklearn*, but here we call it lambda. As we increase lambda we will decrease each one of the coefficients.

### Regularization and Feature Reduction

Feature reduction will most obvious when we work with Lasso regression, as Lasso will drive some of the coefficients in our linear regression down to zero. So, if we may think about a zero-coefficient as removing the contribution of that feature altogether. This would have same effect as manually removing some features prior to modeling. Lasso will find which ones to remove automatically according to some mathematical formula, which we'll see in a bit in Chapter 9. But the first regularization method we will examine is ridge regression.

### Making the Tradeoff

Using Figure 6-4, we see this complexity trade-off and it's possible that variance reduction may actually outpace the increase in bias. So, we may find a better fit model without having to increase bias too much. What we mean here is, we can reduce the complexity while still consistently having enough information to show that relationship between $X$ and $y$ in our training set. So, we're not increasing bias too much, rather we're able to reduce complexity for some time while barely affecting that bias. And this may happen if we're starting with an extremely over fit model, and we can keep reducing variance without increasing the bias. Then we find eventually that optimal value that's going to allow us to have the lowest mean squared error on our holdout set. So, the red "X" marks that spot, as we see variance and bias minimized as well.

![Coefficient value change vs regularization (λ) change.](C:\Users\jeff\Documents\R\Fig_9_04.TIF)
Figure 6-4.

### Lasso Regression Model

## Lasso Regression

LASSO (Least Absolute Shrinkage Selector Operator) is quite like ridge, but we’ll try to understand the difference them by implementing it in our satellite survivability problem. Let’s first discuss some preliminaries.

### Lasso Regression Preliminaries

Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e., models with fewer parameters). This type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.

### L1 Regularization

Lasso regression performs $L^1$ regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, $L^2$ regularization (e.g., Ridge regression) does not result in elimination of coefficients or sparse models. This makes the LASSO far easier to interpret than the Ridge.
The goal of the algorithm is to minimize:

$$\sum_{i=1}^{n}\left( y_{i}-\sum_{j}{x}_{(ij)}β_{j} \right)^2 +λ\sum_{j-1}^{p}|β_{j}|$$ 

Which is the same as minimizing the sum of squares with constraint $\sum|β_j |≤s$. Some of the $β$s are shrunk to exactly zero, resulting in a regression model that’s easier to interpret. So, the basic difference between ridge and lasso is the way we penalize the regression.


```{r, install packages}
if(!require(glmnet)) install.packages("glmnet")
if(!require(superml)) install.packages("superml")
if(!require(rsq)) install.packages("rsq")
if(!require(lars)) install.packages("lars")
if(!require(caret)) install.packages("caret")
```

### Load Imputed Data

```{r}
library(glmnet)
train2 <- read.csv("C:\\Users\\jeff\\Documents\\Data\\survive2.csv")
```

### Data Standardization

Now, we want to keep in mind that with our original linear regression cost function, scaling would not have a large effect on our eventual outcome. But now that we have added on coefficient weights to our cost function, scale will be of utmost importance.

For example, suppose we're working with two variables to predict survivability for a certain resident space objects (RSOs). One of our variables $x_1$ is going to be the altitude of the RSO and the other x_2 variable is going to be the stealth technology used for each RSO. `Orbit_Height` is an integer classification between 1 and 3 and the `RSO_Weight` is an integer between 4.5 kg and 21.4 kg, the coefficient for Survival of 1 unit-change will have a much larger coefficient with regards to its change in altitude, $x_1$, compared to a one-unit change in weight, $x_2$. 

Therefore, if we end up with a higher price coefficient we'll end up being highly penalized for that coefficient. We're going to penalize large coefficients using our new ridge regression. Hence, we need to first ensure that all of our different features are on the same scale. And we can do that using the standardization technique that we see here, just subtracting the mean and dividing by the standard deviation.

$$\hat{x}'=\frac{(x-\bar{x})}{σ}$$

So, let us take or satellite survivability data and standardize it in `R`. We will write the standard variables as $x'$ for each. Later, we'll create a function that does all of the standardization automatically.



```{r}
library(glmnet)
train<-read.csv("C:\\Users\\jeff\\Documents\\Data\\Survive.csv") #Import the train set

train$RSO_Weight[is.na(train$RSO_Weight)] <- mean(train$RSO_Weight, na.rm = TRUE)
train$RSO_Density[is.na(train$RSO_Density)] <- "Low"
train$RSO_Visibility[train$RSO_Visibility == 0] <- mean(train$RSO_Visibility)
train$RSO_MRP[is.na(train$RSO_MRP)] <- mean(train$RSO_MRP, na.rm = TRUE)
train$Orbit_Estab_Year=2013 - train$Orbit_Estab_Year
train$Orbit_Height[is.na(train$Orbit_Height)] <- "LEO"
train$Stealth_Type[is.na(train$Stealth_Type)] <- "Stealth_1"
train$RSO_Type[is.na(train$RSO_Type)] <- "RSO_Type1"
```

### Define the Features and Response Sets
Now, that we have imputed missing values, we need to define the set of features and the response, $Y$.

```{r}
train<-train[c(-1)]
Y<-train[c(11)]
```

### Matricize the Data
Next, we take the response and the features an form a model matrix, $X$.

```{r}
X <- model.matrix(Survivability~., train)
```

### Define Lambda
We also need to define the initial value of lambda and the stop and step values that we'll iterate through, and plot the values.

```{r, fig.width=5, fig.height=3.5, dpi=330}
#lambda <- 10^seq(0, -3, by = -.05)
lambda <- 10^seq(10, -2, length = 100)
plot(lambda, col="green3", lwd=2)
```

### Split the Data into Subsets 
Finally, we split the set into the training set and the cross-validation set, which will complete our data preprocessing.

```{r}
set.seed(567)
part <- sample(2, nrow(X), replace = TRUE, prob = c(0.7, 0.3))
X_train<- X[part == 1,]
X_cv<- X[part == 2,]

X_train <- as.matrix(X_train)
X_cv <- as.matrix(X_cv)

Y_train<- Y[part == 1,]
Y_cv<- Y[part == 2,]
```

## Lasso Regression Model Construction
Here, we also use the `glmnet` function to build the lasso regression model and use it to predict future values.


```{r}
lasso_reg <- glmnet(X_train, Y_train, alpha = 1, lambda = lambda, type.measure = "mse")
bestlam <- lasso_reg$lambda.min
```

```{r, fig.height = 4, fig.width = 6, dpi = 330}
par(mfrow = c(1,1))
lasso_reg_cv = cv.glmnet(X_train, Y_train, alpha = 1)
coef(lasso_reg_cv)
plot(lasso_reg_cv)
```
# MODEL 4: Lasso Regression
We again construct a lasso regression model but with different lambda value settings and a search for the **optimal lambda**.

```{r}
lambdas <- 10^seq(0, -3, by = -.05)
library(glmnet)
cv_lasso <- cv.glmnet(X[X_train,], Y[X_train], alpha = 1, lambda = lambdas)
optimal_lambda <- cv_lasso$lambda.min
optimal_lambda
par(mar=c(4,4,1,1))
plot(lambdas,cv_lasso$cvm,ylab="Mean-Squared Error",xlab="Lambda",  type="l", lwd=3, col="dodgerblue") 
lasso_reg = glmnet(X_train, Y_train,  alpha = 1, family = 'gaussian', lambda = optimal_lambda, thresh = 1e-07)
summary(lasso_reg)
coef(lasso_reg)
```
## Model Evaluation
Here, we write a function to calculate `R-squared` and `RMSE`.

```{r}
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SSR <- sum((true - mean(true))^2)
  SST<-SSR+SSE
  R_square <- SSR / SST
  RMSE = sqrt(SSE/nrow(df))
  return(data.frame(
  RMSE = RMSE,
  Rsquare = R_square
))}
```

# Prediction and evaluation on train data

```{r}
predictions_train <- predict(lasso_reg, s = optimal_lambda, newx = X_train)
res3<-eval_results(Y_train, predictions_train, train)
```

# Prediction and evaluation on test data
```{r}
predictions_cv <- predict(lasso_reg, s = optimal_lambda, newx = X_cv)
res4<-eval_results(Y_cv, predictions_cv, X_cv)
print(paste("Lasso Regression Model =", round(res3,4)))
print(paste("Lasso Cross Validation =", round(res4,4)))
```


```{r, fig.width=6, fig.height=4, dpi=330}
par(mar=c(10,4,2,1))
lasso.coef<-predict(lasso_reg, type = "coefficients", s = bestlam)[1:43,]
barplot(lasso.coef, main="Model 6 Coefficients",ylab="Coefficients",las=2, cex=.75, cex.lab=.75, cex.main=1.25, cex.sub=.75, cex.axis=.75, las=2, col="#acfffd")
```
The results show that Model 6 is the best fit thus far, with a RMSE of approximately 925 and an R-squared of 0.71. This means that the model features explain 71% of the variation in the Survivability measure.

```{r}
library(caret)
V = varImp(lasso_reg, lambda=0.0001)
#Insert new column
s <- nrow(V)
new <- seq(s)
V$Variables <- new
#Remove insignificant Overall importance values
#Insignificant values < median value
#Transform from numerical to logical
V_log<-V>median(V$Overall) 
V1_log<-V_log==TRUE
#Transform to (0,1)
V2=V1_log-FALSE
#Transform to numerical with insignificant = 0
V3=V*V2
#Convert to data frame
V4<-as.data.frame(V3)
#Remove rows containing 0 overall values
V5 <- V4[!(V4$Overall ==0),]
#Convert to data frame
V5<-as.data.frame(V5)
#Rename "V5" column to "Overall"
names(V5)[1] <- paste('Overall')
#Count variable reduction
nrow(V)
nrow(V)-nrow(V5)
```

```{r, fig.width=6, fig.height=4, dpi=330}
my_ggp <- ggplot2::ggplot(V5, aes(x=reorder(rownames(V5),Overall), y=Overall)) +
  geom_point(aes(color=(factor(rownames(V5)))), size=5, alpha=0.6) +
  geom_segment(aes(x=rownames(V5), y=0 , xend=rownames(V5), yend=Overall),
               color='skyblue', size = 1.5) +
  ggtitle("Variable Importance using Lasso Regression") +
  guides(color = guide_legend(title = "Important Variables")) +
  xlab('') +  ylab('Overall Importance') + 
  coord_flip()

my_ggp + theme_light() + 
  theme(axis.title = element_text(size = 14))  +
  theme(axis.text = element_text(size = 12)) +
  theme(plot.title = element_text(size = 14)) +
  theme(legend.title = element_text(size = 13)) +
  theme(legend.text = element_text(size = 11)) 
```

```{r}
V = varImp(lasso_reg, lambda=0.0001)
#Insert new column
s <- nrow(V)
new <- c("Intercept",
        "RSO_Weight",
        "RSO_Density",
        "RSO_Visibility",
        "RSO_Name",
        "RSO_MRP",
        "Orbit_ID",
        "Orbit_Estab_Year",
        "Orbit_Height",
        "Stealth_Type",
        "RSO_Type",
        "Survivability",
        "Orbit_Estab_Year",
        "RSO_Weight",
        "RSO_Density",
        "RSO_Visibility",
        "RSO_Name",
        "RSO_MRP",
        "Orbit_ID",
        "Orbit_Estab_Year",
        "Orbit_Height",
        "Stealth_Type",
        "RSO_Type",
        "Survivability",
        "Orbit_Estab_Year",
        "RSO_Weight",
        "RSO_Density",
        "RSO_Visibility",
        "RSO_Name",
        "RSO_MRP",
        "Orbit_ID",
        "Orbit_Estab_Year",
        "Orbit_Height",
        "Stealth_Type",
        "RSO_Type",
        "Survivability",
        "Orbit_Estab_Year",
        "RSO_Weight",
        "RSO_Density",
        "RSO_Visibility",
        "RSO_Name",
        "RSO_MRP",
        "Orbit_Identifier")

V$Variables <- new
```
### Print Lasso Regressoin Coefficients

```{r}
lasso.coef<-predict(lasso_reg, type = "coefficients", s = bestlam)[1:43,]
print(paste("Variable: Intercept =",lasso.coef[1]))
print(paste("Variable:", names(train),"=",lasso.coef[2:43]))
```

### Lasso Model Performance

```{r}
eval_results <- function(true, predicted, df) {
  SST <- sum((predicted-true)^2)
  SSE <- sum((true-mean(true))^2)
  R_square <- 1-SSE/SST
  RMSE = sqrt(SSE/nrow(df))
  return(data.frame(
  RMSE = RMSE,
  Rsquare = R_square
))}
```

### Cross-Validation Performance

```{r}
lasso_pred_cv1 <- predict(lasso_reg, s = optimal_lambda, newx = X_train)
lasso_pred_cv2 <- predict(cv_lasso, s = optimal_lambda, newx = X_cv)
res1<-eval_results(Y_cv, lasso_pred_cv1, X_cv)
res2<-eval_results(Y_cv, lasso_pred_cv2, X_cv)
print(paste("Lasso Regression Model =", res1))
print(paste("Lasso Cross Validation =", res2))
```

### Model Comparison Metrics Table

```{r}
# Prediction and evaluation on train data.

predictions_cv1 <- predict(model1, newx = X_cv)
res1 <- eval_results(Y_cv, predictions_cv1, X_cv)
print(paste("model-1 RMSE:", round(res1$RMSE,4), "| Model-1 R2:", round(res1$Rsquare,4)))

predictions_cv2 <- predict(model2, newx = X_cv)
res2 <- eval_results(Y_cv, predictions_cv2, X_cv)
print(paste("model-2 RMSE:", round(res2$RMSE,4), "| Model-2 R2:", round(res2$Rsquare,4)))

predictions_cv3 <- predict(model3x, newx = X_cv)
res3 <- eval_results(Y_cv, predictions_cv3, X_cv)
print(paste("model-3 RMSE:", round(res3$RMSE,4), "| Model-3 R2:", round(res3$Rsquare,4))) 

predictions_cv4 <- predict(cv_lasso, newx = X_cv)
res4 <- eval_results(Y_cv, predictions_cv4, X_cv)
print(paste("model-4 RMSE:", round(res4$RMSE,4), "| Model-4 R2:", round(res4$Rsquare,4))) 
```

```{r}
library(Metrics)
RMSE1 <- rmse(Y_cv, predict_1)
R.sq1 <- rsq(model1)
RMSE2 <- rmse(Y_cv, predict_2)
R.sq2 <- rsq(model2)
RMSE3 <- rmse(Y_cv, predict_3)
R.sq3 <- rsq(model3x)
pred4 <- predict(lasso_reg, s = optimal_lambda, newx = X_cv)
res4<-eval_results(Y_cv, pred4, X_cv)
print(paste("Mod1 R-square =", round(R.sq1,4),"| Mod1 RMSE =", round(RMSE1,4)))
print(paste("Mod2 R-square =", round(R.sq2,4),"| Mod2 RMSE =", round(RMSE2,4)))
print(paste("Mod3 R-square =", round(R.sq3,4),"| Mod3 RMSE =", round(RMSE3,4)))
print(paste("Mod4 R-square =", round(res4$Rsquare,4),"| Mod4 RMSE =", round(res4$RMSE,4)))
```

## Chapter Summary

In this chapter, we discussed estimating and interpreting parameters of a multiple regression model. We saw how to produce predicted values and perform residual analysis. We examined the coefficient of determination, R^2, and adjusted R^2 as model metrics. We also explored significance testing of model features. Next, we looked at examples of linear models with numeric features and linear models with mixed feature types.

```{r}
predictions_cv <- predict(lasso_reg, s = optimal_lambda, newx = X_cv)
res4<-eval_results(Y_cv, predictions_cv, X_cv)
print(paste("Lasso Regression Model =", round(res3,4)))
print(paste("Lasso Cross Validation =", round(res4,4)))
```

